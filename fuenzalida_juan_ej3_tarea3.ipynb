{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb4588f",
   "metadata": {},
   "source": [
    "# Explicacion de \"Generative Pre-trained Transformer\" (GPT) Entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd271cc",
   "metadata": {},
   "source": [
    "colocari introduccion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd8464",
   "metadata": {},
   "source": [
    "üß† Explicaci√≥n paso a paso del algoritmo GPT (versi√≥n resumida)\n",
    "\n",
    "---\n",
    "\n",
    "1. Ejecuci√≥n principal (`run.py`)\n",
    "\n",
    "- Es el punto de entrada del programa.\n",
    "- Usa un argumento `mode` para decidir qu√© acci√≥n ejecutar:\n",
    "  - **`preprocess`** ‚Üí prepara los datos de texto.\n",
    "  - **`train`** ‚Üí entrena o actualiza el modelo GPT.\n",
    "  - **`chat`** ‚Üí inicia una conversaci√≥n interactiva con el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "2. Preprocesamiento de datos (`preprocess.py`)\n",
    "\n",
    "1. Lee el archivo de chat (`assets/input/chat.txt`).\n",
    "2. Elimina caracteres poco frecuentes (como emojis o s√≠mbolos raros).\n",
    "3. Divide los mensajes en tuplas: *(fecha, contacto, mensaje)*.\n",
    "4. Crea *tokens especiales* con los nombres de los contactos y el token de fin `<END>`.\n",
    "5. Tokeniza el texto respetando los tokens especiales.\n",
    "6. Reemplaza tokens muy raros por `<UNK>` para reducir el vocabulario.\n",
    "7. Genera el **vocabulario** (lista √∫nica de tokens).\n",
    "8. Codifica los tokens a tensores num√©ricos.\n",
    "9. Divide los datos en:\n",
    "   - **Entrenamiento:** 90%\n",
    "   - **Validaci√≥n:** 10%\n",
    "10. Guarda los tensores y archivos auxiliares:\n",
    "    - `train.pt`, `valid.pt`, `vocab.txt`, `contacts.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "3. Definici√≥n del modelo (`model.py`)\n",
    "\n",
    "Implementa una versi√≥n simplificada de **GPT (Transformer Decoder)** con los siguientes componentes:\n",
    "\n",
    "- **`Head`** ‚Üí Aplica *self-attention* sobre el contexto.\n",
    "- **`MultiHeadAttention`** ‚Üí Ejecuta varias cabezas de atenci√≥n en paralelo.\n",
    "- **`FeedForward`** ‚Üí Red densa que transforma la salida de la atenci√≥n.\n",
    "- **`Block`** ‚Üí Combina atenci√≥n y red feed-forward con normalizaci√≥n residual.\n",
    "- **`GPTLanguageModel`** ‚Üí Une todos los bloques, agrega embeddings y produce predicciones finales.\n",
    "\n",
    "Incluye un m√©todo:\n",
    "- **`generate()`** ‚Üí Genera texto token por token hasta encontrar el token `<END>`.\n",
    "\n",
    "---\n",
    "\n",
    "4. Entrenamiento del modelo (`train.py`)\n",
    "\n",
    "1. Carga los tensores de entrenamiento y validaci√≥n.\n",
    "2. Carga el vocabulario (`vocab.txt`).\n",
    "3. Si `--update` est√° activado:\n",
    "   - Carga un modelo existente (`model.pt`) para continuar su entrenamiento.\n",
    "   - Si no, crea un modelo nuevo desde cero.\n",
    "4. Define el optimizador **AdamW** con una tasa de aprendizaje (`learn_rate`).\n",
    "5. Ciclo de entrenamiento (`max_iters` iteraciones):\n",
    "   - Obtiene un **batch** de datos con `get_batch()`.\n",
    "   - Calcula la **p√©rdida (loss)** entre predicciones y objetivos.\n",
    "   - Realiza retropropagaci√≥n y actualiza los pesos.\n",
    "   - Cada `eval_interval` pasos, eval√∫a la p√©rdida en *train* y *validaci√≥n* (`estimate_loss`).\n",
    "6. Guarda el modelo entrenado en `assets/models/model.pt`.\n",
    "\n",
    "---\n",
    "\n",
    "5. Chat interactivo (`chat.py`)\n",
    "\n",
    "1. Carga el modelo GPT entrenado y el vocabulario.\n",
    "2. Pide al usuario un mensaje de entrada.\n",
    "3. El modelo genera una respuesta token a token.\n",
    "4. Muestra la salida usando `print_delayed()` (efecto de escritura lenta).\n",
    "5. Contin√∫a la conversaci√≥n hasta que el usuario escribe `<END>`.\n",
    "\n",
    "---\n",
    "\n",
    "6. Funciones auxiliares (`utils.py`)\n",
    "\n",
    "| Funci√≥n | Descripci√≥n |\n",
    "|----------|--------------|\n",
    "| `encode` / `decode` | Convierte entre texto y tensores de √≠ndices. |\n",
    "| `get_batch` | Crea lotes de secuencias para el entrenamiento. |\n",
    "| `estimate_loss` | Calcula la p√©rdida promedio del modelo. |\n",
    "| `custom_tokenizer` | Tokeniza texto conservando tokens especiales. |\n",
    "| `get_vocab` | Genera una lista de tokens √∫nicos. |\n",
    "| `print_delayed` | Imprime texto con un efecto de tipeo. |\n",
    "| `current_time` | Muestra la hora actual (para logs). |\n",
    "\n",
    "---\n",
    "\n",
    "‚öôÔ∏è Resumen general\n",
    "\n",
    "1. **Entrada:** Chat de WhatsApp en texto plano.  \n",
    "2. **Preprocesamiento:** Limpieza, tokenizaci√≥n y codificaci√≥n num√©rica.  \n",
    "3. **Entrenamiento:** Ajuste de un modelo GPT (transformer decoder) con esos datos.  \n",
    "4. **Generaci√≥n:** El modelo aprende a responder mensajes en el mismo estilo.  \n",
    "5. **Interfaz:** Chat interactivo donde el modelo responde autom√°ticamente.\n",
    "\n",
    "---\n",
    "\n",
    "> üí¨ En s√≠ntesis, este proyecto implementa un mini-GPT capaz de aprender patrones de conversaci√≥n a partir de chats reales, y luego simular respuestas coherentes y personalizadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a02ed",
   "metadata": {},
   "source": [
    "## run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a537c69",
   "metadata": {},
   "source": [
    "El archivo run.py es el punto de entrada principal del proyecto y funciona como una interfaz de l√≠nea de comandos (CLI) que permite ejecutar las distintas fases del pipeline del modelo GPT: el preprocesamiento de datos, el entrenamiento y el modo de conversaci√≥n. Para lograr esto, utiliza el m√≥dulo est√°ndar de Python llamado argparse, que se encarga de interpretar los argumentos que el usuario escribe al ejecutar el script. En las primeras l√≠neas se importan los m√≥dulos necesarios: argparse para gestionar los argumentos y los tres subm√≥dulos del proyecto ubicados en src ‚Äîchat, preprocess y train‚Äî, cada uno responsable de una parte distinta del proceso.\n",
    "La funci√≥n principal main() comienza creando un objeto ArgumentParser que define los par√°metros que el usuario puede pasar por consola. Luego, agrega un argumento posicional obligatorio llamado \"mode\", que solo acepta tres valores posibles: \"preprocess\", \"train\" o \"chat\". Este argumento determina qu√© acci√≥n realizar√° el programa. Adem√°s, se define un argumento opcional --update, que act√∫a como una bandera booleana (True si se incluye en el comando y False en caso contrario). Este flag se usa para indicar si, durante el entrenamiento, el modelo debe continuar a partir de un modelo previamente guardado en lugar de iniciar desde cero.\n",
    "Una vez definidos los argumentos, se ejecuta parser.parse_args(), que interpreta los par√°metros escritos en la l√≠nea de comandos y los guarda en un objeto args. A continuaci√≥n, mediante una estructura condicional, el programa ejecuta la funci√≥n correspondiente seg√∫n el valor de args.mode: si el usuario eligi√≥ \"preprocess\", se llama a preprocess.make_train_test(), que se encarga de tokenizar el texto, generar el vocabulario y crear los conjuntos de entrenamiento y validaci√≥n; si eligi√≥ \"train\", se invoca a train.model_training(args.update), que entrena el modelo (desde cero o continuando, dependiendo de la bandera --update); y si eligi√≥ \"chat\", se llama a chat.conversation(), que carga el modelo entrenado y permite mantener un di√°logo con √©l generando texto de forma autoregresiva.\n",
    "Finalmente, el bloque if __name__ == \"__main__\": main() asegura que esta funci√≥n se ejecute solo cuando el archivo se ejecuta directamente desde la terminal (por ejemplo, python run.py train) y no cuando es importado desde otro m√≥dulo. En conjunto, run.py act√∫a como un controlador o ‚Äúmen√∫ principal‚Äù que organiza el flujo completo del proyecto. Gracias a esta estructura, el usuario puede realizar todas las operaciones principales ‚Äîpreprocesamiento, entrenamiento y conversaci√≥n‚Äî mediante simples comandos en la terminal, sin necesidad de modificar el c√≥digo fuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from src import chat, preprocess, train\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"mode\", choices=[\"preprocess\", \"train\", \"chat\"], help=\"The mode to be execute.\")\n",
    "    parser.add_argument(\"--update\", action=\"store_true\", help=\"Flag when model shall be updated based on current parameters\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == \"preprocess\":\n",
    "        preprocess.make_train_test()\n",
    "    elif args.mode == \"train\":\n",
    "        train.model_training(args.update)\n",
    "    elif args.mode == \"chat\":\n",
    "        chat.conversation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c693556",
   "metadata": {},
   "source": [
    "## config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09682425",
   "metadata": {},
   "source": [
    "El archivo config.py act√∫a como un contenedor central de par√°metros y configuraciones que controlan el comportamiento del modelo, del entrenamiento y del preprocesamiento. Su funci√≥n es evitar que los valores clave est√©n ‚Äúduros‚Äù dentro del c√≥digo, lo que permite modificar la arquitectura o los hiperpar√°metros de forma simple y coherente desde un solo lugar. En este caso, el archivo define tres grupos principales de configuraciones: par√°metros del modelo, par√°metros del entrenamiento y par√°metros del preprocesamiento o codificaci√≥n.\n",
    "\n",
    "En la primera secci√≥n, ‚Äúmodel hyperparameters‚Äù, se establecen los valores que determinan la estructura del modelo tipo GPT.\n",
    "\n",
    "block_size = 32 indica la longitud m√°xima del contexto o ventana de tokens que el modelo usa para predecir el siguiente elemento. En otras palabras, el modelo solo ‚Äúve‚Äù los √∫ltimos 32 tokens del texto a la vez, lo que define su capacidad de memoria a corto plazo.\n",
    "\n",
    "embed_size = 256 define la dimensi√≥n del espacio vectorial en el que se representar√°n los tokens despu√©s del proceso de embedding; cada palabra o s√≠mbolo del vocabulario se convierte en un vector de 256 valores.\n",
    "\n",
    "dropout = 0.2 es la tasa de abandono utilizada para evitar el sobreajuste, lo que significa que el 20 % de las conexiones del modelo se desactivan aleatoriamente durante el entrenamiento.\n",
    "\n",
    "n_heads = 6 especifica que la atenci√≥n multi-cabeza del Transformer estar√° compuesta por seis cabezas, lo que permite que el modelo atienda simult√°neamente a distintas partes del contexto.\n",
    "\n",
    "n_layer = 6 indica que el modelo tendr√° seis bloques o capas de Transformer apilados, cada uno compuesto por mecanismos de atenci√≥n y redes feed-forward.\n",
    "\n",
    "eval_iters = 200 indica cu√°ntos lotes (batches) se utilizar√°n durante la evaluaci√≥n para estimar la p√©rdida promedio en los conjuntos de entrenamiento y validaci√≥n.\n",
    "\n",
    "batch_size = 32 determina cu√°ntas secuencias se procesan en paralelo en cada iteraci√≥n de entrenamiento.\n",
    "\n",
    "En la segunda secci√≥n, ‚Äúlearning hyperparameters‚Äù, se definen los par√°metros que afectan directamente la din√°mica del entrenamiento.\n",
    "\n",
    "learn_rate = 3e-4 (0.0003) es la tasa de aprendizaje del optimizador, que controla el tama√±o de los pasos en la actualizaci√≥n de los pesos.\n",
    "\n",
    "max_iters = 5000 representa el n√∫mero m√°ximo de iteraciones de entrenamiento (pasos de optimizaci√≥n).\n",
    "\n",
    "eval_interval = 500 indica que cada 500 iteraciones se realizar√° una evaluaci√≥n del modelo en los conjuntos de datos de entrenamiento y validaci√≥n para monitorear su desempe√±o.\n",
    "\n",
    "La tercera parte, ‚Äúpreprocess‚Äù, contiene configuraciones del procesamiento del texto.\n",
    "\n",
    "min_count_chars = 1 y min_count_tokens = 1 establecen la frecuencia m√≠nima que debe tener un car√°cter o token en el corpus para ser incluido en el vocabulario. En este caso, cualquier token que aparezca al menos una vez se mantiene, lo que equivale a no filtrar por frecuencia.\n",
    "\n",
    "Finalmente, en la secci√≥n ‚Äúencoding‚Äù, se definen los tokens especiales usados para representar situaciones particulares dentro del texto.\n",
    "\n",
    "end_token = \"<END>\" marca el final de una secuencia o conversaci√≥n, ayudando al modelo a entender d√≥nde termina una respuesta.\n",
    "\n",
    "unknown_token = \"<UNK>\" se usa para reemplazar cualquier palabra o s√≠mbolo que no est√© en el vocabulario (token desconocido).\n",
    "\n",
    "n_chats = 5 probablemente determina el n√∫mero m√°ximo de turnos de conversaci√≥n o la cantidad de mensajes previos que el modelo puede considerar en modo ‚Äúchat‚Äù.\n",
    "\n",
    "En conjunto, este archivo config.py proporciona la base que gu√≠a la construcci√≥n del modelo, su entrenamiento y el tratamiento de los datos. Centralizar estos valores facilita la reproducibilidad de los experimentos y permite ajustar r√°pidamente la complejidad del modelo o la escala del entrenamiento sin modificar el c√≥digo fuente de los m√≥dulos principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4772acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "block_size = 32\n",
    "embed_size = 256\n",
    "dropout = 0.2\n",
    "n_heads = 6\n",
    "n_layer = 6\n",
    "eval_iters = 200\n",
    "batch_size = 32\n",
    "\n",
    "# learning hyperparameters\n",
    "learn_rate = 3e-4\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "\n",
    "# preprocess\n",
    "min_count_chars = 1\n",
    "min_count_tokens = 1\n",
    "\n",
    "# encoding\n",
    "end_token = \"<END>\"\n",
    "unknown_token = \"<UNK>\"\n",
    "n_chats = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f7d69",
   "metadata": {},
   "source": [
    "## chat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df251a8",
   "metadata": {},
   "source": [
    "El archivo chat.py implementa el modo de conversaci√≥n del modelo GPT previamente entrenado, simulando un chat interactivo entre el usuario y el modelo. Su objetivo es cargar el modelo entrenado y su vocabulario, luego generar respuestas autom√°ticamente a los mensajes que el usuario escriba, manteniendo un contexto de di√°logo.\n",
    "\n",
    "El script comienza importando varios m√≥dulos. json se utiliza para leer los archivos que contienen el vocabulario y la lista de contactos; random permite seleccionar aleatoriamente un contacto durante la simulaci√≥n; y torch es la librer√≠a principal para cargar el modelo de lenguaje entrenado y manejar tensores. Luego, se importa prompt y WordCompleter desde prompt_toolkit, una librer√≠a que mejora la interacci√≥n en consola, ofreciendo autocompletado de palabras mientras el usuario escribe (por ejemplo, los nombres de contactos o el token de fin). Posteriormente se importan desde config.py los valores end_token (el token que indica el final de la conversaci√≥n) y n_chats (el n√∫mero de turnos autom√°ticos que el modelo genera antes de volver a pedir entrada al usuario). Finalmente, desde src.utils se traen las funciones auxiliares:\n",
    "\n",
    "custom_tokenizer: tokeniza el texto de entrada teniendo en cuenta los tokens especiales,\n",
    "\n",
    "encode y decode: convierten entre texto y tensores num√©ricos seg√∫n el vocabulario,\n",
    "\n",
    "print_delayed: imprime texto de manera pausada, simulando que el modelo ‚Äúescribe‚Äù como una persona.\n",
    "\n",
    "La funci√≥n principal, conversation(), es la encargada de orquestar el chat. Primero abre y lee el archivo \"assets/output/vocab.txt\", que contiene el vocabulario del modelo en formato JSON, y lo carga como un diccionario vocab. Luego hace lo mismo con \"assets/output/contacts.txt\", una lista de posibles nombres o interlocutores del chat, guard√°ndolos en contacts. A partir de esta informaci√≥n, crea la lista spec_tokens, que combina los nombres de contacto y el token especial de fin (<END>). A continuaci√≥n, se carga el modelo de lenguaje con torch.load(\"assets/models/model.pt\"), lo que restaura el estado del modelo entrenado (una instancia de GPTLanguageModel). Tambi√©n se crea un objeto WordCompleter con spec_tokens, que permite que el autocompletado en la consola sugiera autom√°ticamente los contactos o el token <END> cuando el usuario escribe.\n",
    "\n",
    "El primer prompt solicita al usuario un mensaje inicial con la etiqueta \"message >> \". El texto ingresado se almacena en la variable input. A la vez, se crea un tensor vac√≠o llamado output, que representar√° la secuencia de tokens generada por el modelo durante la conversaci√≥n.\n",
    "\n",
    "A partir de ah√≠, se entra en un bucle while input != end_token: que se ejecuta hasta que el usuario escriba el token <END>, lo que detiene la conversaci√≥n. Dentro del bucle, hay otro ciclo for _ in range(n_chats): que controla cu√°ntas respuestas autom√°ticas generar√° el modelo antes de volver a solicitar una entrada del usuario (por ejemplo, si n_chats = 5, el modelo generar√° cinco mensajes seguidos).\n",
    "\n",
    "En cada iteraci√≥n del ciclo, el texto del usuario (input) se pasa a custom_tokenizer, que lo divide en tokens respetando los tokens especiales como los nombres de contacto o <END>. Luego, con encode(add_tokens, vocab), esos tokens se convierten a √≠ndices num√©ricos seg√∫n el vocabulario cargado, obteniendo un tensor add_context. Este tensor se concatena con output (que acumula el historial de conversaci√≥n) mediante torch.cat((output, add_context)), y se reorganiza con .unsqueeze(1).T para ajustar sus dimensiones al formato que espera el modelo: una secuencia de tama√±o (1, longitud) en lugar de (longitud,).\n",
    "\n",
    "Despu√©s, se guarda la longitud actual de output en n0, y se llama a model.generate(context, vocab), que usa el modelo para generar nuevos tokens a partir del contexto actual. El resultado, un tensor m√°s largo que contiene la nueva predicci√≥n, se guarda nuevamente en output. La diferencia n1 - n0 permite identificar cu√°ntos tokens nuevos fueron generados. Con decode(output[n0-n1:], vocab) se transforman esos tokens nuevos en texto legible, y print_delayed los imprime en pantalla simulando la escritura progresiva del modelo. Para el siguiente turno, la variable input se actualiza eligiendo aleatoriamente un nuevo contacto desde la lista contacts, lo que crea la sensaci√≥n de que el modelo conversa con distintos interlocutores.\n",
    "\n",
    "Una vez que el bucle de n_chats termina, el programa vuelve a pedir una nueva entrada del usuario mediante prompt(\"\\nresponse >> \", completer=completer, default=\"\"). Este nuevo mensaje se procesa nuevamente en el siguiente ciclo while. Si el usuario introduce el token <END>, la condici√≥n input != end_token deja de cumplirse y la conversaci√≥n se detiene.\n",
    "\n",
    "En resumen, chat.py convierte el modelo GPT entrenado en una experiencia de chat interactiva en consola. Lee el vocabulario y los contactos, carga el modelo, permite que el usuario escriba mensajes con autocompletado y genera respuestas autom√°ticas en base al contexto previo. Su estructura combina procesamiento de texto, generaci√≥n autoregresiva con el modelo y una interfaz de usuario sencilla, todo ello gestionado en un ciclo continuo que se detiene solo cuando el usuario decide finalizar la conversaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd472b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from prompt_toolkit import prompt\n",
    "from prompt_toolkit.completion import WordCompleter\n",
    "\n",
    "from config import end_token, n_chats\n",
    "from src.utils import custom_tokenizer, decode, encode, print_delayed\n",
    "\n",
    "\n",
    "def conversation() -> None:\n",
    "    \"\"\"\n",
    "    Emulates chat conversations by sampling from a pre-trained GPTLanguageModel.\n",
    "\n",
    "    This function loads a trained GPTLanguageModel along with vocabulary and \n",
    "    the list of special tokens. It then enters into a loop where the user specifies \n",
    "    a contact. Given this input, the model generates a sample response. The conversation \n",
    "    continues until the user inputs the end token.\n",
    "\n",
    "    :example:\n",
    "\n",
    "    >>> conversation()\n",
    "    message >> Alice\n",
    "    Model's Response: How are you?\n",
    "    response >> end\n",
    "    \"\"\"\n",
    "    with open(\"assets/output/vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = json.loads(f.read())\n",
    "\n",
    "    with open(\"assets/output/contacts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        contacts = json.loads(f.read())   \n",
    "\n",
    "    spec_tokens = contacts + [end_token]\n",
    "    model = torch.load(\"assets/models/model.pt\")\n",
    "    completer = WordCompleter(spec_tokens, ignore_case=True)\n",
    "    \n",
    "    input = prompt(\"message >> \", completer=completer, default=\"\")\n",
    "    output = torch.tensor([], dtype=torch.long)\n",
    "    print()\n",
    "\n",
    "    while input != end_token:\n",
    "        for _ in range(n_chats):\n",
    "\n",
    "            add_tokens = custom_tokenizer(input, spec_tokens)\n",
    "            add_context = encode(add_tokens, vocab)\n",
    "            context = torch.cat((output, add_context)).unsqueeze(1).T\n",
    "\n",
    "            n0 = len(output)\n",
    "            output = model.generate(context, vocab)\n",
    "            n1 = len(output)\n",
    "\n",
    "            print_delayed(decode(output[n0-n1:], vocab))\n",
    "            input = random.choice(contacts)\n",
    "\n",
    "        input = prompt(\"\\nresponse >> \", completer=completer, default=\"\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e8a62",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c52373d",
   "metadata": {},
   "source": [
    "El archivo model.py implementa desde cero una versi√≥n simplificada de un modelo GPT (Generative Pretrained Transformer) utilizando PyTorch, siguiendo la arquitectura de los transformadores. Est√° compuesto por distintos m√≥dulos que se combinan jer√°rquicamente para procesar secuencias de texto y generar predicciones token por token.\n",
    "Primero, la clase Head define una cabeza de atenci√≥n individual, que aplica el mecanismo de self-attention: para cada token calcula consultas (queries), claves (keys) y valores (values), genera una matriz de atenci√≥n con q @ k·µÄ / ‚àöd, aplica una m√°scara triangular inferior para evitar mirar tokens futuros (causalidad), y produce una salida ponderada de los valores.\n",
    "La clase MultiHeadAttention combina varias cabezas de atenci√≥n en paralelo (seg√∫n n_heads), concatena sus salidas y las pasa por una capa lineal, replicando el mecanismo de atenci√≥n m√∫ltiple del transformer original.\n",
    "Luego, la clase FeedForward aplica dos capas lineales con activaci√≥n ReLU y dropout, expandiendo y luego reduciendo la dimensionalidad del embedding; act√∫a como una red no lineal por token.\n",
    "La clase Block representa un bloque completo del transformador, compuesto por atenci√≥n multi-cabeza seguida de la red feedforward, con residual connections (sumas con la entrada) y LayerNorm antes de cada subm√≥dulo, que estabilizan el entrenamiento.\n",
    "La clase principal GPTLanguageModel construye el modelo completo: incluye embeddings de tokens y de posici√≥n (para dar informaci√≥n del orden en la secuencia), una pila secuencial de n_layer bloques transformer, y una capa final lineal para proyectar al tama√±o del vocabulario y obtener los logits de probabilidad sobre el siguiente token. Adem√°s, inicializa los pesos de manera normal con desviaci√≥n est√°ndar baja para estabilidad.\n",
    "En el m√©todo forward, el modelo toma una matriz de √≠ndices de tokens (idx), obtiene sus embeddings, les suma los embeddings posicionales, los pasa por los bloques y produce los logits. Si se proporcionan targets, calcula la p√©rdida de entrop√≠a cruzada.\n",
    "Finalmente, el m√©todo generate implementa la generaci√≥n de texto autoregresiva: dado un contexto inicial (idx), predice el siguiente token, lo a√±ade a la secuencia y repite hasta encontrar el token de fin (<END>), evitando seleccionar el token desconocido (<UNK>). As√≠, el archivo model.py contiene toda la definici√≥n estructural del modelo GPT, desde los mecanismos b√°sicos de atenci√≥n hasta el proceso de generaci√≥n secuencial de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from config import (block_size, dropout, embed_size, end_token, n_heads,\n",
    "                    n_layer, unknown_token)\n",
    "from src.utils import encode\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This module performs self-attention operations on the input tensor, producing \n",
    "    an output tensor with the same time-steps but different channels. \n",
    "    \n",
    "    :param head_size: The size of the head in the multi-head attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)                                     # (B, T, head_size)\n",
    "        q = self.query(x)                                   # (B, T, head_size)\n",
    "\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2,-1)                        # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei /= math.sqrt(k.shape[-1])                       # (B, T, T)\n",
    "        \n",
    "        # avoid look-ahead\n",
    "        tril = torch.tril(torch.ones(T, T))\n",
    "        wei = wei.masked_fill(tril == 0, float(\"-inf\"))     # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)                        # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # weighted aggregation of the values\n",
    "        v = self.value(x)                                   # (B, T, head_size)\n",
    "        out = wei @ v                                       # (B, T, T) @ (B, T, hs) -> (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains multiple `Head` objects, which perform self-attention \n",
    "    operations in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # list of parallel heads that are concatenated by the linear layer in the end\n",
    "        head_size = embed_size // n_heads\n",
    "        heads_list = [Head(head_size) for _ in range(n_heads)]\n",
    "        \n",
    "        self.heads = nn.ModuleList(heads_list)\n",
    "        self.linear = nn.Linear(n_heads * head_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads_list = [h(x) for h in self.heads]\n",
    "        out = torch.cat(heads_list, dim=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"\n",
    "    This module passes the input tensor through a series of linear transformations \n",
    "    and non-linear activations.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # factor of 4 is the multiplier of nodes\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    This module contains a single transformer block, which consists of multi-head \n",
    "    self-attention followed by feed-forward neural networks.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sa = MultiHeadAttention()\n",
    "        self.ffwd = FeedFoward()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encompasses the entire GPT model, including the token and position embeddings, \n",
    "    multiple transformer blocks, and output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding tables for token and their positioning in the context\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embedding = nn.Embedding(block_size, embed_size)\n",
    "        \n",
    "        # put one block after the other sequentially (not parallel like multi-head attention)\n",
    "        block_list = [Block() for _ in range(n_layer)]\n",
    "        self.blocks = nn.Sequential(*block_list)\n",
    "        \n",
    "        # output layer after sequential blocks\n",
    "        self.ln_output = nn.LayerNorm(embed_size)\n",
    "        self.linear_output = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        # initialize weights and biases for linear layers and embeddings\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "            # The linear layers in self-attention do not have a biases\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding(idx)                     # (B, T, C)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T))           # (T, C)\n",
    "        x = tok_emb + pos_emb                                   # (B, T, C)\n",
    "        x = self.blocks(x)                                      # (B, T, C)\n",
    "        x = self.ln_output(x)                                   # (B, T, C)\n",
    "        logits = self.linear_output(x)                          # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, vocab):\n",
    "        \n",
    "        # Initialize idx_net for while loop\n",
    "        idx_next = torch.zeros(1)\n",
    "        idx_end = encode([end_token], vocab)\n",
    "        idx_unk = encode([unknown_token], vocab)\n",
    "\n",
    "        # continue to sample tokens until special end token\n",
    "        while idx_next[0] != idx_end:\n",
    "\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last block_size tokens for each batch (row)\n",
    "            idx_cond = idx[:, -block_size:]                     # (B, T)\n",
    "\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_cond)                          # (B, T, vocab_size)\n",
    "            logits = logits[:, -1, :]                           # (B, vocab_size)            \n",
    "            probs = F.softmax(logits, dim=-1)                   # (B, vocab_size)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # when the sampled token is UNK, then sample again\n",
    "            while idx_next[0] == idx_unk:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "                \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)             # (B, T+1)\n",
    "\n",
    "        # output everything except the end token\n",
    "        return idx[0][:-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a442c",
   "metadata": {},
   "source": [
    "## preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a729d4",
   "metadata": {},
   "source": [
    "El archivo preprocess.py se encarga de preparar los datos de entrenamiento y validaci√≥n a partir de un chat de WhatsApp exportado, para que puedan ser usados por el modelo GPT definido en el proyecto. Su flujo principal est√° en la funci√≥n make_train_test(), que realiza todo el proceso de limpieza, tokenizaci√≥n y codificaci√≥n del texto.\n",
    "Primero, el script abre el archivo assets/input/chat.txt que contiene las conversaciones. Luego, identifica los caracteres poco frecuentes (como emojis o s√≠mbolos raros) mediante la funci√≥n get_infrequent_tokens, que usa un contador (Counter) para marcar aquellos que aparecen menos veces que un umbral (min_count_chars). Esos caracteres infrecuentes se eliminan del texto con drop_chars.\n",
    "Despu√©s, usa una expresi√≥n regular (re.findall) para dividir el chat en tuplas con la estructura (fecha, contacto, mensaje) y descarta los mensajes del sistema (que empiezan con \\u200e). Los nombres de los contactos se convierten a min√∫sculas y se tratan como tokens especiales, agreg√°ndoles un ‚Äú:‚Äù al final para distinguirlos.\n",
    "La funci√≥n flatten_tuple combina esas tuplas en un solo texto plano, donde cada mensaje termina con el token especial de fin (<END>). Luego se tokeniza el texto con custom_tokenizer, que probablemente divide el texto en palabras o subunidades considerando los tokens especiales definidos.\n",
    "Posteriormente, se detectan los tokens poco frecuentes con get_infrequent_tokens (esta vez a nivel de palabra/token, usando min_count_tokens como umbral) y se reemplazan por el token <UNK> mediante mask_tokens, reduciendo as√≠ el tama√±o del vocabulario.\n",
    "Una vez limpio y tokenizado, se genera el vocabulario con get_vocab, que asigna un √≠ndice entero a cada token. Con esa codificaci√≥n, los tokens se transforman a tensores num√©ricos con encode. Los datos resultantes se dividen en un 90 % para entrenamiento y un 10 % para validaci√≥n, y se guardan como archivos de PyTorch (train.pt y valid.pt).\n",
    "Finalmente, se exportan tambi√©n los archivos vocab.txt (el diccionario de tokens) y contacts.txt (los nombres de los emisores del chat) en formato JSON. As√≠, preprocess.py automatiza toda la etapa de preparaci√≥n de datos, dejando listos los tensores y los metadatos que el modelo GPT utilizar√° para aprender a predecir y generar mensajes similares al estilo del chat original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Set, Tuple, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from config import end_token, min_count_chars, min_count_tokens, unknown_token\n",
    "from src.utils import custom_tokenizer, encode, get_vocab\n",
    "\n",
    "\n",
    "def get_infrequent_tokens(tokens: Union[List[str], str], min_count: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Identify tokens that appear less than a minimum count.\n",
    "    \n",
    "    :param tokens: When it is the raw text in a string, frequencies are counted on character level.\n",
    "                   When it is the tokenized corpus as list, frequencies are counted on token level.\n",
    "    :min_count: Threshold of occurence to flag a token.\n",
    "    :return: List of tokens that appear infrequently. \n",
    "    \"\"\"\n",
    "    counts = Counter(tokens)\n",
    "    infreq_tokens = set([k for k,v in counts.items() if v<=min_count])\n",
    "    return infreq_tokens\n",
    "\n",
    "\n",
    "def mask_tokens(tokens: List[str], mask: Set[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Iterate through all tokens. Any token that is part of the set, is replaced by the unknown token.\n",
    "\n",
    "    :param tokens: The tokenized corpus.\n",
    "    :param mask: Set of tokens that shall be masked in the corpus.\n",
    "    :return: List of tokenized corpus after the masking operation.\n",
    "    \"\"\"\n",
    "    return [t.replace(t, unknown_token) if t in mask else t for t in tokens]\n",
    "\n",
    "\n",
    "def drop_chars(txt: str, drop: Set[str]) -> str:\n",
    "    \"\"\"Drop a list of characters from string\"\"\"\n",
    "\n",
    "    return txt.translate(str.maketrans(\"\", \"\", \"\".join(drop)))\n",
    "\n",
    "\n",
    "def flatten_tuple(txt: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"Convert list of tuples into string separated by the end token\"\"\"\n",
    "\n",
    "    return \"\".join([x0+\":\"+x1+end_token for x0, x1 in txt])\n",
    "\n",
    "\n",
    "def make_train_test() -> None:\n",
    "    \"\"\"\n",
    "    Prepare training and testing datasets from chat messages. This function performs multiple tasks:\n",
    "    \n",
    "    1. Reads a corpus of WhatsApp chat messages from a text file\n",
    "    2. Filters out infrequent characters from the corpus\n",
    "    3. Splits the text based on regular expressions\n",
    "    4. Tokenizes the text and encodes the tokens into integers\n",
    "    5. Splits the encoded data into training and validation sets\n",
    "    6. Saves the training and validation datasets, as well as the vocab and senders, to disk\n",
    "    \"\"\"\n",
    "    with open(\"assets/input/chat.txt\", \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # remove very rare characters (mostly emojies)\n",
    "    infreq_chars = get_infrequent_tokens(text, min_count=min_count_chars)\n",
    "    text = drop_chars(text, infreq_chars)\n",
    "\n",
    "    # split string into list of tuples (date, contact, message)\n",
    "    pattern = r'\\[(.*?)\\] (.*?): (.*)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    text = [(x1, x2.lower()) for x0, x1, x2 in matches if not x2.startswith(\"\\u200e\")]\n",
    "\n",
    "    # get list of all contacts, treated as special tokens\n",
    "    contacts = list(set([contact+\":\" for contact, msg in text]))\n",
    "    spec_tokens = contacts + [end_token]\n",
    "\n",
    "    # convert list of tuples into list of tokens (word or character level)\n",
    "    text_flat = flatten_tuple(text)\n",
    "    tokens = custom_tokenizer(txt=text_flat, spec_tokens=spec_tokens)\n",
    "\n",
    "    # mask very rare tokens as unknown, to shrink the vocabulary\n",
    "    infreq_tokens = get_infrequent_tokens(tokens, min_count=min_count_tokens)\n",
    "    tokens = mask_tokens(tokens, infreq_tokens)\n",
    "\n",
    "    # get vocabulary of corpus to file\n",
    "    vocab = get_vocab(tokens)\n",
    "    print(f\"The corpus has {len(vocab)} unique tokens.\")\n",
    "\n",
    "    # encode tokens into a tensor of integers\n",
    "    data = encode(tokens, vocab)\n",
    "\n",
    "    # split up the data into train and validation set\n",
    "    n = int(0.9*len(data))\n",
    "    train_data = data[:n]\n",
    "    valid_data = data[n:]\n",
    "\n",
    "    # export tensors\n",
    "    torch.save(train_data, \"assets/output/train.pt\")\n",
    "    torch.save(valid_data, \"assets/output/valid.pt\")\n",
    "\n",
    "    with open(\"assets/output/vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    with open(\"assets/output/contacts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(contacts))\n",
    "\n",
    "    print(\"SUCCESS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada2b0c",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4274e6e",
   "metadata": {},
   "source": [
    "El archivo train.py contiene el m√≥dulo encargado de entrenar o actualizar el modelo GPT usando los datos procesados previamente. Su funci√≥n principal, model_training(update: bool), controla todo el flujo de entrenamiento.\n",
    "Primero, el script carga los tensores de entrenamiento y validaci√≥n (train.pt y valid.pt) junto con el vocabulario (vocab.txt) que contiene el mapeo entre tokens y sus √≠ndices num√©ricos. Luego, seg√∫n el valor del par√°metro update, decide si inicia un nuevo modelo (GPTLanguageModel) o carga uno previamente entrenado desde assets/models/model.pt para continuar el aprendizaje. Esto permite tanto un entrenamiento desde cero como una reanudaci√≥n incremental.\n",
    "Despu√©s de definir el modelo, se crea un optimizador AdamW con una tasa de aprendizaje (learn_rate) definida en el archivo config.py. El script tambi√©n calcula y muestra el n√∫mero total de par√°metros entrenables del modelo para tener una idea del tama√±o de la red.\n",
    "En el bucle principal de entrenamiento, que se ejecuta durante max_iters iteraciones, cada cierto n√∫mero de pasos (determinado por eval_interval), se eval√∫a el desempe√±o del modelo midiendo la p√©rdida (loss) en los conjuntos de entrenamiento y validaci√≥n usando la funci√≥n auxiliar estimate_loss. Esta funci√≥n realiza una evaluaci√≥n r√°pida sin actualizar los pesos, permitiendo monitorear si el modelo mejora o se sobreajusta.\n",
    "En cada iteraci√≥n, se extrae un lote de ejemplos mediante get_batch(train_data), que selecciona fragmentos de texto (secuencias de tokens) y sus correspondientes objetivos de predicci√≥n. Luego, el modelo calcula los logits y la p√©rdida (loss) comparando sus predicciones con los objetivos reales.\n",
    "La p√©rdida se retropropaga con loss.backward(), el optimizador actualiza los pesos (optimizer.step()), y antes de cada paso se limpian los gradientes acumulados con optimizer.zero_grad(). Este ciclo de c√°lculo, retropropagaci√≥n y actualizaci√≥n se repite hasta completar todas las iteraciones.\n",
    "Finalmente, el modelo entrenado se guarda en disco (model.pt) para poder usarse despu√©s en la generaci√≥n de texto (por ejemplo, en el modo ‚Äúchat‚Äù).\n",
    "En resumen, train.py implementa todo el proceso de entrenamiento supervisado del modelo GPT, gestionando desde la carga de datos y configuraci√≥n hasta la optimizaci√≥n iterativa y el guardado del modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e839519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from config import eval_interval, learn_rate, max_iters\n",
    "from src.model import GPTLanguageModel\n",
    "from src.utils import current_time, estimate_loss, get_batch\n",
    "\n",
    "\n",
    "def model_training(update: bool) -> None:\n",
    "    \"\"\"\n",
    "    Trains or updates a GPTLanguageModel using pre-loaded data.\n",
    "\n",
    "    This function either initializes a new model or loads an existing model based\n",
    "    on the `update` parameter. It then trains the model using the AdamW optimizer\n",
    "    on the training and validation data sets. Finally the trained model is saved.\n",
    "\n",
    "    :param update: Boolean flag to indicate whether to update an existing model.\n",
    "    \"\"\"\n",
    "    # LOAD DATA -----------------------------------------------------------------\n",
    "\n",
    "    train_data = torch.load(\"assets/output/train.pt\")\n",
    "    valid_data = torch.load(\"assets/output/valid.pt\")\n",
    "\n",
    "    with open(\"assets/output/vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = json.loads(f.read())\n",
    "\n",
    "    # INITIALIZE / LOAD MODEL ---------------------------------------------------\n",
    "\n",
    "    if update:\n",
    "        try:\n",
    "            model = torch.load(\"assets/models/model.pt\")\n",
    "            print(\"Loaded existing model to continue training.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No existing model found. Initializing a new model.\")\n",
    "            model = GPTLanguageModel(vocab_size=len(vocab))\n",
    "        \n",
    "    else:\n",
    "        print(\"Initializing a new model.\")\n",
    "        model = GPTLanguageModel(vocab_size=len(vocab))\n",
    "\n",
    "    # initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learn_rate)\n",
    "\n",
    "    # number of model parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters to be optimized: {n_params}\\n\", )\n",
    "\n",
    "    # MODEL TRAINING ------------------------------------------------------------\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        # evaluate the loss on train and valid sets every 'eval_interval' steps\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "            train_loss = estimate_loss(model, train_data)\n",
    "            valid_loss = estimate_loss(model, valid_data)\n",
    "\n",
    "            time = current_time()\n",
    "            print(f\"{time} | step {i}: train loss {train_loss:.4f}, valid loss {valid_loss:.4f}\")\n",
    "\n",
    "        # sample batch of data\n",
    "        x_batch, y_batch = get_batch(train_data)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.save(model, \"assets/models/model.pt\")\n",
    "    print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e30afb",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834945ce",
   "metadata": {},
   "source": [
    "El archivo utils.py re√∫ne un conjunto de funciones auxiliares fundamentales para el funcionamiento general del sistema GPT, cubriendo desde el manejo de datos y tokenizaci√≥n hasta la evaluaci√≥n del modelo y la impresi√≥n de resultados.\n",
    "La funci√≥n estimate_loss(model, data) eval√∫a el rendimiento del modelo sin modificar sus pesos. Para ello, cambia el modo del modelo a evaluaci√≥n (model.eval()), calcula la p√©rdida en varios lotes (eval_iters veces) usando la funci√≥n get_batch, promedia los resultados y luego devuelve el modelo a modo de entrenamiento (model.train()). Esto permite medir la calidad del aprendizaje sin interferir con el entrenamiento.\n",
    "La funci√≥n get_batch(data) crea peque√±os lotes de entrada y salida a partir de los datos codificados. Genera √≠ndices aleatorios dentro del tensor completo y toma fragmentos de longitud block_size. El tensor x contiene los tokens actuales, y y es el mismo desplazado una posici√≥n hacia adelante, es decir, los targets que el modelo debe predecir.\n",
    "La funci√≥n encode(s, vocab) transforma una lista de tokens en un tensor de √≠ndices enteros, usando el vocabulario. Si un token no se encuentra, se reemplaza por el token especial <UNK> (definido en config.py). Si ese token no existe en el vocabulario (por ejemplo, en datos nuevos), se le asigna un √≠ndice aleatorio, garantizando que todos los elementos tengan representaci√≥n num√©rica.\n",
    "La inversa, decode(tensor, vocab), convierte un tensor de √≠ndices nuevamente en texto legible, usando un mapeo inverso de √≠ndices a tokens y uni√©ndolos con espacios.\n",
    "El custom_tokenizer es un tokenizador configurable basado en RegexpTokenizer de NLTK. Su patr√≥n por defecto divide el texto en palabras o caracteres, pero adem√°s protege los tokens especiales (spec_tokens) ‚Äîcomo nombres de contactos o el token de fin <END>‚Äî para que se mantengan intactos durante la segmentaci√≥n.\n",
    "La funci√≥n get_vocab(text) genera una lista ordenada de los tokens √∫nicos del corpus, que se usa para construir el diccionario de vocabulario del modelo.\n",
    "current_time() devuelve la hora actual en formato HH:MM:SS, usada principalmente para registrar avances durante el entrenamiento.\n",
    "Finalmente, print_delayed(s, delay) imprime una cadena car√°cter por car√°cter con una peque√±a pausa configurable, simulando una respuesta ‚Äúen tiempo real‚Äù durante el modo de chat interactivo.\n",
    "En conjunto, este m√≥dulo proporciona las herramientas de soporte esenciales para el preprocesamiento, la codificaci√≥n de texto, la evaluaci√≥n del modelo y la interfaz interactiva, sirviendo de base para la comunicaci√≥n entre los distintos componentes del sistema GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from config import batch_size, block_size, eval_iters, unknown_token\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, data):\n",
    "    \"\"\"\n",
    "    Set evaluation mode and evaluate the loss on multiple batches. \n",
    "    Return the average of collected losses.\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    loss_list = torch.zeros(eval_iters)\n",
    "    \n",
    "    for i in range(eval_iters):\n",
    "        X, Y = get_batch(data)\n",
    "        logits, loss = model(X, Y)\n",
    "        loss_list[i] = loss.item()\n",
    "\n",
    "    loss_avg = loss_list.mean()    \n",
    "    model.train() \n",
    "    return loss_avg\n",
    "\n",
    "\n",
    "def get_batch(data):\n",
    "    \"\"\"Generate a small batch of data of inputs x and targets y\"\"\"\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def encode(s: list, vocab: list) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Encode a list of tokens into a tensor of integers, given a fixed vocabulary. \n",
    "    When a token is not found in the vocabulary, the special unknown token is assigned. \n",
    "    When the training set did not use that special token, a random token is assigned.\n",
    "    \"\"\"\n",
    "    rand_token = random.randint(0, len(vocab))\n",
    "\n",
    "    map = {s:i for i,s in enumerate(vocab)}\n",
    "    enc = [map.get(c, map.get(unknown_token, rand_token)) for c in s]\n",
    "    enc = torch.tensor(enc, dtype=torch.long)\n",
    "    return enc\n",
    "\n",
    "\n",
    "def decode(tensor: torch.tensor, vocab: list) -> str:\n",
    "    \"\"\"Decode a tensor of integers, back into a string.\"\"\"\n",
    "\n",
    "    map_enc = {s:i for i,s in enumerate(vocab)}\n",
    "    map_dec = {i:s for s,i in map_enc.items()}\n",
    "    dec = [map_dec[i.item()] for i in tensor]\n",
    "    dec = \" \".join(dec)\n",
    "    return dec\n",
    "\n",
    "\n",
    "def custom_tokenizer(txt: str, spec_tokens: List[str], pattern: str=\"|\\d|\\\\w+|[^\\\\s]\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text into words or characters using NLTK's RegexpTokenizer, considerung \n",
    "    given special combinations as single tokens.\n",
    "\n",
    "    :param txt: The corpus as a single string element.\n",
    "    :param spec_tokens: A list of special tokens (e.g. ending, out-of-vocab).\n",
    "    :param pattern: By default the corpus is tokenized on a word level (split by spaces).\n",
    "                    Numbers are considered single tokens.\n",
    "\n",
    "    >> note: The pattern for character level tokenization is '|.'\n",
    "    \"\"\"\n",
    "    pattern = \"|\".join(spec_tokens) + pattern\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_vocab(text: Union[List[str], str]) -> List[str]:\n",
    "    \"\"\"Returns a sorted list of all unique tokens in the corpus.\"\"\"\n",
    "\n",
    "    return sorted(list(set(text)))\n",
    "\n",
    "\n",
    "def current_time():\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "def print_delayed(s: str, delay: float = 0.05) -> None:\n",
    "    \"\"\"\n",
    "    Prints each character of a string one by one on the same line with a delay.\n",
    "\n",
    "    :param s: The input string.\n",
    "    :param delay: The time delay between each character in seconds.\n",
    "    \"\"\"\n",
    "    for char in s:\n",
    "        print(char, end=\"\", flush=True)\n",
    "        time.sleep(delay)\n",
    "\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

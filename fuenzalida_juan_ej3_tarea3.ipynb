{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb4588f",
   "metadata": {},
   "source": [
    "# Explicacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a02ed",
   "metadata": {},
   "source": [
    "## run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from src import chat, preprocess, train\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"mode\", choices=[\"preprocess\", \"train\", \"chat\"], help=\"The mode to be execute.\")\n",
    "    parser.add_argument(\"--update\", action=\"store_true\", help=\"Flag when model shall be updated based on current parameters\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == \"preprocess\":\n",
    "        preprocess.make_train_test()\n",
    "    elif args.mode == \"train\":\n",
    "        train.model_training(args.update)\n",
    "    elif args.mode == \"chat\":\n",
    "        chat.conversation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c693556",
   "metadata": {},
   "source": [
    "## config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4772acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "block_size = 32\n",
    "embed_size = 256\n",
    "dropout = 0.2\n",
    "n_heads = 6\n",
    "n_layer = 6\n",
    "eval_iters = 200\n",
    "batch_size = 32\n",
    "\n",
    "# learning hyperparameters\n",
    "learn_rate = 3e-4\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "\n",
    "# preprocess\n",
    "min_count_chars = 1\n",
    "min_count_tokens = 1\n",
    "\n",
    "# encoding\n",
    "end_token = \"<END>\"\n",
    "unknown_token = \"<UNK>\"\n",
    "n_chats = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f7d69",
   "metadata": {},
   "source": [
    "## chat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd472b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from prompt_toolkit import prompt\n",
    "from prompt_toolkit.completion import WordCompleter\n",
    "\n",
    "from config import end_token, n_chats\n",
    "from src.utils import custom_tokenizer, decode, encode, print_delayed\n",
    "\n",
    "\n",
    "def conversation() -> None:\n",
    "    \"\"\"\n",
    "    Emulates chat conversations by sampling from a pre-trained GPTLanguageModel.\n",
    "\n",
    "    This function loads a trained GPTLanguageModel along with vocabulary and \n",
    "    the list of special tokens. It then enters into a loop where the user specifies \n",
    "    a contact. Given this input, the model generates a sample response. The conversation \n",
    "    continues until the user inputs the end token.\n",
    "\n",
    "    :example:\n",
    "\n",
    "    >>> conversation()\n",
    "    message >> Alice\n",
    "    Model's Response: How are you?\n",
    "    response >> end\n",
    "    \"\"\"\n",
    "    with open(\"assets/output/vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = json.loads(f.read())\n",
    "\n",
    "    with open(\"assets/output/contacts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        contacts = json.loads(f.read())   \n",
    "\n",
    "    spec_tokens = contacts + [end_token]\n",
    "    model = torch.load(\"assets/models/model.pt\")\n",
    "    completer = WordCompleter(spec_tokens, ignore_case=True)\n",
    "    \n",
    "    input = prompt(\"message >> \", completer=completer, default=\"\")\n",
    "    output = torch.tensor([], dtype=torch.long)\n",
    "    print()\n",
    "\n",
    "    while input != end_token:\n",
    "        for _ in range(n_chats):\n",
    "\n",
    "            add_tokens = custom_tokenizer(input, spec_tokens)\n",
    "            add_context = encode(add_tokens, vocab)\n",
    "            context = torch.cat((output, add_context)).unsqueeze(1).T\n",
    "\n",
    "            n0 = len(output)\n",
    "            output = model.generate(context, vocab)\n",
    "            n1 = len(output)\n",
    "\n",
    "            print_delayed(decode(output[n0-n1:], vocab))\n",
    "            input = random.choice(contacts)\n",
    "\n",
    "        input = prompt(\"\\nresponse >> \", completer=completer, default=\"\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e8a62",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from config import (block_size, dropout, embed_size, end_token, n_heads,\n",
    "                    n_layer, unknown_token)\n",
    "from src.utils import encode\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This module performs self-attention operations on the input tensor, producing \n",
    "    an output tensor with the same time-steps but different channels. \n",
    "    \n",
    "    :param head_size: The size of the head in the multi-head attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)                                     # (B, T, head_size)\n",
    "        q = self.query(x)                                   # (B, T, head_size)\n",
    "\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2,-1)                        # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei /= math.sqrt(k.shape[-1])                       # (B, T, T)\n",
    "        \n",
    "        # avoid look-ahead\n",
    "        tril = torch.tril(torch.ones(T, T))\n",
    "        wei = wei.masked_fill(tril == 0, float(\"-inf\"))     # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)                        # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # weighted aggregation of the values\n",
    "        v = self.value(x)                                   # (B, T, head_size)\n",
    "        out = wei @ v                                       # (B, T, T) @ (B, T, hs) -> (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains multiple `Head` objects, which perform self-attention \n",
    "    operations in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # list of parallel heads that are concatenated by the linear layer in the end\n",
    "        head_size = embed_size // n_heads\n",
    "        heads_list = [Head(head_size) for _ in range(n_heads)]\n",
    "        \n",
    "        self.heads = nn.ModuleList(heads_list)\n",
    "        self.linear = nn.Linear(n_heads * head_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads_list = [h(x) for h in self.heads]\n",
    "        out = torch.cat(heads_list, dim=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"\n",
    "    This module passes the input tensor through a series of linear transformations \n",
    "    and non-linear activations.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # factor of 4 is the multiplier of nodes\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    This module contains a single transformer block, which consists of multi-head \n",
    "    self-attention followed by feed-forward neural networks.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sa = MultiHeadAttention()\n",
    "        self.ffwd = FeedFoward()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encompasses the entire GPT model, including the token and position embeddings, \n",
    "    multiple transformer blocks, and output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding tables for token and their positioning in the context\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embedding = nn.Embedding(block_size, embed_size)\n",
    "        \n",
    "        # put one block after the other sequentially (not parallel like multi-head attention)\n",
    "        block_list = [Block() for _ in range(n_layer)]\n",
    "        self.blocks = nn.Sequential(*block_list)\n",
    "        \n",
    "        # output layer after sequential blocks\n",
    "        self.ln_output = nn.LayerNorm(embed_size)\n",
    "        self.linear_output = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        # initialize weights and biases for linear layers and embeddings\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "            # The linear layers in self-attention do not have a biases\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding(idx)                     # (B, T, C)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T))           # (T, C)\n",
    "        x = tok_emb + pos_emb                                   # (B, T, C)\n",
    "        x = self.blocks(x)                                      # (B, T, C)\n",
    "        x = self.ln_output(x)                                   # (B, T, C)\n",
    "        logits = self.linear_output(x)                          # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, vocab):\n",
    "        \n",
    "        # Initialize idx_net for while loop\n",
    "        idx_next = torch.zeros(1)\n",
    "        idx_end = encode([end_token], vocab)\n",
    "        idx_unk = encode([unknown_token], vocab)\n",
    "\n",
    "        # continue to sample tokens until special end token\n",
    "        while idx_next[0] != idx_end:\n",
    "\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            # crop idx to the last block_size tokens for each batch (row)\n",
    "            idx_cond = idx[:, -block_size:]                     # (B, T)\n",
    "\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_cond)                          # (B, T, vocab_size)\n",
    "            logits = logits[:, -1, :]                           # (B, vocab_size)            \n",
    "            probs = F.softmax(logits, dim=-1)                   # (B, vocab_size)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # when the sampled token is UNK, then sample again\n",
    "            while idx_next[0] == idx_unk:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "                \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)             # (B, T+1)\n",
    "\n",
    "        # output everything except the end token\n",
    "        return idx[0][:-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a442c",
   "metadata": {},
   "source": [
    "## preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Set, Tuple, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from config import end_token, min_count_chars, min_count_tokens, unknown_token\n",
    "from src.utils import custom_tokenizer, encode, get_vocab\n",
    "\n",
    "\n",
    "def get_infrequent_tokens(tokens: Union[List[str], str], min_count: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Identify tokens that appear less than a minimum count.\n",
    "    \n",
    "    :param tokens: When it is the raw text in a string, frequencies are counted on character level.\n",
    "                   When it is the tokenized corpus as list, frequencies are counted on token level.\n",
    "    :min_count: Threshold of occurence to flag a token.\n",
    "    :return: List of tokens that appear infrequently. \n",
    "    \"\"\"\n",
    "    counts = Counter(tokens)\n",
    "    infreq_tokens = set([k for k,v in counts.items() if v<=min_count])\n",
    "    return infreq_tokens\n",
    "\n",
    "\n",
    "def mask_tokens(tokens: List[str], mask: Set[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Iterate through all tokens. Any token that is part of the set, is replaced by the unknown token.\n",
    "\n",
    "    :param tokens: The tokenized corpus.\n",
    "    :param mask: Set of tokens that shall be masked in the corpus.\n",
    "    :return: List of tokenized corpus after the masking operation.\n",
    "    \"\"\"\n",
    "    return [t.replace(t, unknown_token) if t in mask else t for t in tokens]\n",
    "\n",
    "\n",
    "def drop_chars(txt: str, drop: Set[str]) -> str:\n",
    "    \"\"\"Drop a list of characters from string\"\"\"\n",
    "\n",
    "    return txt.translate(str.maketrans(\"\", \"\", \"\".join(drop)))\n",
    "\n",
    "\n",
    "def flatten_tuple(txt: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"Convert list of tuples into string separated by the end token\"\"\"\n",
    "\n",
    "    return \"\".join([x0+\":\"+x1+end_token for x0, x1 in txt])\n",
    "\n",
    "\n",
    "def make_train_test() -> None:\n",
    "    \"\"\"\n",
    "    Prepare training and testing datasets from chat messages. This function performs multiple tasks:\n",
    "    \n",
    "    1. Reads a corpus of WhatsApp chat messages from a text file\n",
    "    2. Filters out infrequent characters from the corpus\n",
    "    3. Splits the text based on regular expressions\n",
    "    4. Tokenizes the text and encodes the tokens into integers\n",
    "    5. Splits the encoded data into training and validation sets\n",
    "    6. Saves the training and validation datasets, as well as the vocab and senders, to disk\n",
    "    \"\"\"\n",
    "    with open(\"assets/input/chat.txt\", \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # remove very rare characters (mostly emojies)\n",
    "    infreq_chars = get_infrequent_tokens(text, min_count=min_count_chars)\n",
    "    text = drop_chars(text, infreq_chars)\n",
    "\n",
    "    # split string into list of tuples (date, contact, message)\n",
    "    pattern = r'\\[(.*?)\\] (.*?): (.*)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    text = [(x1, x2.lower()) for x0, x1, x2 in matches if not x2.startswith(\"\\u200e\")]\n",
    "\n",
    "    # get list of all contacts, treated as special tokens\n",
    "    contacts = list(set([contact+\":\" for contact, msg in text]))\n",
    "    spec_tokens = contacts + [end_token]\n",
    "\n",
    "    # convert list of tuples into list of tokens (word or character level)\n",
    "    text_flat = flatten_tuple(text)\n",
    "    tokens = custom_tokenizer(txt=text_flat, spec_tokens=spec_tokens)\n",
    "\n",
    "    # mask very rare tokens as unknown, to shrink the vocabulary\n",
    "    infreq_tokens = get_infrequent_tokens(tokens, min_count=min_count_tokens)\n",
    "    tokens = mask_tokens(tokens, infreq_tokens)\n",
    "\n",
    "    # get vocabulary of corpus to file\n",
    "    vocab = get_vocab(tokens)\n",
    "    print(f\"The corpus has {len(vocab)} unique tokens.\")\n",
    "\n",
    "    # encode tokens into a tensor of integers\n",
    "    data = encode(tokens, vocab)\n",
    "\n",
    "    # split up the data into train and validation set\n",
    "    n = int(0.9*len(data))\n",
    "    train_data = data[:n]\n",
    "    valid_data = data[n:]\n",
    "\n",
    "    # export tensors\n",
    "    torch.save(train_data, \"assets/output/train.pt\")\n",
    "    torch.save(valid_data, \"assets/output/valid.pt\")\n",
    "\n",
    "    with open(\"assets/output/vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "    with open(\"assets/output/contacts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(contacts))\n",
    "\n",
    "    print(\"SUCCESS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada2b0c",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e839519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from config import eval_interval, learn_rate, max_iters\n",
    "from src.model import GPTLanguageModel\n",
    "from src.utils import current_time, estimate_loss, get_batch\n",
    "\n",
    "\n",
    "def model_training(update: bool) -> None:\n",
    "    \"\"\"\n",
    "    Trains or updates a GPTLanguageModel using pre-loaded data.\n",
    "\n",
    "    This function either initializes a new model or loads an existing model based\n",
    "    on the `update` parameter. It then trains the model using the AdamW optimizer\n",
    "    on the training and validation data sets. Finally the trained model is saved.\n",
    "\n",
    "    :param update: Boolean flag to indicate whether to update an existing model.\n",
    "    \"\"\"\n",
    "    # LOAD DATA -----------------------------------------------------------------\n",
    "\n",
    "    train_data = torch.load(\"assets/output/train.pt\")\n",
    "    valid_data = torch.load(\"assets/output/valid.pt\")\n",
    "\n",
    "    with open(\"assets/output/vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = json.loads(f.read())\n",
    "\n",
    "    # INITIALIZE / LOAD MODEL ---------------------------------------------------\n",
    "\n",
    "    if update:\n",
    "        try:\n",
    "            model = torch.load(\"assets/models/model.pt\")\n",
    "            print(\"Loaded existing model to continue training.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No existing model found. Initializing a new model.\")\n",
    "            model = GPTLanguageModel(vocab_size=len(vocab))\n",
    "        \n",
    "    else:\n",
    "        print(\"Initializing a new model.\")\n",
    "        model = GPTLanguageModel(vocab_size=len(vocab))\n",
    "\n",
    "    # initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learn_rate)\n",
    "\n",
    "    # number of model parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters to be optimized: {n_params}\\n\", )\n",
    "\n",
    "    # MODEL TRAINING ------------------------------------------------------------\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        # evaluate the loss on train and valid sets every 'eval_interval' steps\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "            train_loss = estimate_loss(model, train_data)\n",
    "            valid_loss = estimate_loss(model, valid_data)\n",
    "\n",
    "            time = current_time()\n",
    "            print(f\"{time} | step {i}: train loss {train_loss:.4f}, valid loss {valid_loss:.4f}\")\n",
    "\n",
    "        # sample batch of data\n",
    "        x_batch, y_batch = get_batch(train_data)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.save(model, \"assets/models/model.pt\")\n",
    "    print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e30afb",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from config import batch_size, block_size, eval_iters, unknown_token\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, data):\n",
    "    \"\"\"\n",
    "    Set evaluation mode and evaluate the loss on multiple batches. \n",
    "    Return the average of collected losses.\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    loss_list = torch.zeros(eval_iters)\n",
    "    \n",
    "    for i in range(eval_iters):\n",
    "        X, Y = get_batch(data)\n",
    "        logits, loss = model(X, Y)\n",
    "        loss_list[i] = loss.item()\n",
    "\n",
    "    loss_avg = loss_list.mean()    \n",
    "    model.train() \n",
    "    return loss_avg\n",
    "\n",
    "\n",
    "def get_batch(data):\n",
    "    \"\"\"Generate a small batch of data of inputs x and targets y\"\"\"\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def encode(s: list, vocab: list) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Encode a list of tokens into a tensor of integers, given a fixed vocabulary. \n",
    "    When a token is not found in the vocabulary, the special unknown token is assigned. \n",
    "    When the training set did not use that special token, a random token is assigned.\n",
    "    \"\"\"\n",
    "    rand_token = random.randint(0, len(vocab))\n",
    "\n",
    "    map = {s:i for i,s in enumerate(vocab)}\n",
    "    enc = [map.get(c, map.get(unknown_token, rand_token)) for c in s]\n",
    "    enc = torch.tensor(enc, dtype=torch.long)\n",
    "    return enc\n",
    "\n",
    "\n",
    "def decode(tensor: torch.tensor, vocab: list) -> str:\n",
    "    \"\"\"Decode a tensor of integers, back into a string.\"\"\"\n",
    "\n",
    "    map_enc = {s:i for i,s in enumerate(vocab)}\n",
    "    map_dec = {i:s for s,i in map_enc.items()}\n",
    "    dec = [map_dec[i.item()] for i in tensor]\n",
    "    dec = \" \".join(dec)\n",
    "    return dec\n",
    "\n",
    "\n",
    "def custom_tokenizer(txt: str, spec_tokens: List[str], pattern: str=\"|\\d|\\\\w+|[^\\\\s]\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text into words or characters using NLTK's RegexpTokenizer, considerung \n",
    "    given special combinations as single tokens.\n",
    "\n",
    "    :param txt: The corpus as a single string element.\n",
    "    :param spec_tokens: A list of special tokens (e.g. ending, out-of-vocab).\n",
    "    :param pattern: By default the corpus is tokenized on a word level (split by spaces).\n",
    "                    Numbers are considered single tokens.\n",
    "\n",
    "    >> note: The pattern for character level tokenization is '|.'\n",
    "    \"\"\"\n",
    "    pattern = \"|\".join(spec_tokens) + pattern\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_vocab(text: Union[List[str], str]) -> List[str]:\n",
    "    \"\"\"Returns a sorted list of all unique tokens in the corpus.\"\"\"\n",
    "\n",
    "    return sorted(list(set(text)))\n",
    "\n",
    "\n",
    "def current_time():\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "def print_delayed(s: str, delay: float = 0.05) -> None:\n",
    "    \"\"\"\n",
    "    Prints each character of a string one by one on the same line with a delay.\n",
    "\n",
    "    :param s: The input string.\n",
    "    :param delay: The time delay between each character in seconds.\n",
    "    \"\"\"\n",
    "    for char in s:\n",
    "        print(char, end=\"\", flush=True)\n",
    "        time.sleep(delay)\n",
    "\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
